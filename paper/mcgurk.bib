@article{Zeiler2012,
abstract = {We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment.},
archivePrefix = {arXiv},
arxivId = {1212.5701},
author = {Zeiler, Matthew D.},
eprint = {1212.5701},
file = {:Users/darrylhannan/Library/Application Support/Mendeley Desktop/Downloaded/Zeiler - 2012 - ADADELTA An Adaptive Learning Rate Method(2).pdf:pdf},
journal = {CoRR},
month = {dec},
title = {{ADADELTA: An Adaptive Learning Rate Method}},
url = {http://arxiv.org/abs/1212.5701},
year = {2012}
}
@article{Kim2017,
archivePrefix = {arXiv},
arxivId = {1711.07998},
author = {Kim, Edward and Hannan, Darryl and Kenyon, Garrett},
eprint = {1711.07998},
file = {:Users/darrylhannan/Library/Application Support/Mendeley Desktop/Downloaded/Kim, Hannan, Kenyon - 2017 - Deep Sparse Coding for Invariant Multimodal Halle Berry Neurons.pdf:pdf},
month = {nov},
title = {{Deep Sparse Coding for Invariant Multimodal Halle Berry Neurons}},
url = {https://arxiv.org/abs/1711.07998},
year = {2017}
}
@article{Nair2010,
abstract = {Restricted Boltzmann machines were devel-oped using binary stochastic hidden units. These can be generalized by replacing each binary unit by an infinite number of copies that all have the same weights but have pro-gressively more negative biases. The learning and inference rules for these " Stepped Sig-moid Units " are unchanged. They can be ap-proximated efficiently by noisy, rectified lin-ear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face verification on the Labeled Faces in the Wild dataset. Unlike binary units, rectified linear units preserve information about relative in-tensities as information travels through mul-tiple layers of feature detectors.},
author = {Nair, Vinod and Hinton, Geoffrey E},
file = {:Users/darrylhannan/Library/Application Support/Mendeley Desktop/Downloaded/Nair, Hinton - Unknown - Rectified Linear Units Improve Restricted Boltzmann Machines.pdf:pdf},
journal = {Proc. 27th International Conference on Machine Learning},
title = {{Rectified Linear Units Improve Restricted Boltzmann Machines}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.165.6419{\&}rep=rep1{\&}type=pdf},
year = {2010}
}
@article{Chung2017,
abstract = {The goal of this work is to recognise phrases and sen-tences being spoken by a talking face, with or without the audio. Unlike previous works that have focussed on recog-nising a limited number of words or phrases, we tackle lip reading as an open-world problem – unconstrained natural language sentences, and in the wild videos. Our key contributions are: (1) a 'Watch, Listen, Attend and Spell' (WLAS) network that learns to transcribe videos of mouth motion to characters; (2) a curriculum learning strategy to accelerate training and to reduce overfitting; (3) a 'Lip Reading Sentences' (LRS) dataset for visual speech recognition, consisting of over 100,000 natural sentences from British television. The WLAS model trained on the LRS dataset surpasses the performance of all previous work on standard lip read-ing benchmark datasets, often by a significant margin. This lip reading performance beats a professional lip reader on videos from BBC television, and we also demonstrate that visual information helps to improve speech recognition per-formance even when the audio is available.},
author = {Chung, Joon Son and Senior, Andrew and Vinyals, Oriol and Zisserman, Andrew},
file = {:Users/darrylhannan/Library/Application Support/Mendeley Desktop/Downloaded/Chung et al. - Unknown - Lip Reading Sentences in the Wild.pdf:pdf},
journal = {IEEE Conference on Computer Vision and Pattern Recognition},
title = {{Lip Reading Sentences in the Wild}},
url = {https://arxiv.org/pdf/1611.05358.pdf},
year = {2017}
}
@article{Torfi2017,
abstract = {—Audio-visual recognition (AVR) has been considered as a solution for speech recognition tasks when the audio is corrupted, as well as a visual recognition method used for speaker verification in multi-speaker scenarios. The approach of AVR systems is to leverage the extracted information from one modality to improve the recognition ability of the other modality by complementing the missing information. The essen-tial problem is to find the correspondence between the audio and visual streams, which is the goal of this work. We propose the use of a coupled 3D Convolutional Neural Network (3D-CNN) architecture that can map both modalities into a representation space to evaluate the correspondence of audio-visual streams using the learned multimodal features. The proposed architecture will incorporate both spatial and temporal information jointly to effectively find the correlation between temporal information for different modalities. By using a relatively small network architecture and much smaller dataset for training, our proposed method surpasses the performance of the existing similar methods for audio-visual matching which use 3D CNNs for feature rep-resentation. We also demonstrate that an effective pair selection method can significantly increase the performance. The proposed method achieves relative improvements over 20{\%} on the Equal Error Rate (EER) and over 7{\%} on the Average Precision (AP) in comparison to the state-of-the-art method.},
author = {Torfi, Amirsina and {Mehdi Iranmanesh}, Seyed and Nasrabadi, Nasser and Dawson, Jeremy},
file = {:Users/darrylhannan/Library/Application Support/Mendeley Desktop/Downloaded/Torfi et al. - Unknown - 3D Convolutional Neural Networks for Cross Audio-Visual Matching Recognition.pdf:pdf},
journal = {IEEE Access},
keywords = {3D Architecture,Audio-visual Recognition,Deep Learning,Index Terms—Convolutional Networks},
title = {{3D Convolutional Neural Networks for Cross Audio-Visual Matching Recognition}},
url = {https://arxiv.org/pdf/1706.05739.pdf},
volume = {5},
year = {2017}
}
@article{Sporea2010,
abstract = {The current study investigates the McGurk effect by modelling it with neural networks. The simulations are designed to test the two main theories about the moment at which the auditory-visual integration happens. To further analyze the causes behind the McGurk illusion, the neural network that best models the effect is used to simulate the influence of language and the frequency of phonemes on auditory-visual speech perception, using two phonetic distribution from English and Japanese, with different empirical results in the McGurk effect.},
author = {Sporea, Ioana and Gruning, Andre},
file = {:Users/darrylhannan/Library/Application Support/Mendeley Desktop/Downloaded/Sporea, Gruning - Unknown - Modelling the McGurk effect(2).pdf:pdf},
journal = {European Symposium on Artificial Neural Networks},
title = {{Modelling the McGurk effect}},
url = {https://www.elen.ucl.ac.be/Proceedings/esann/esannpdf/es2010-61.pdf},
year = {2010}
}
@article{Getz2017,
abstract = {Adult speech perception is generally enhanced when information is provided from multiple modalities. In contrast, infants do not appear to benefit from combining auditory and visual speech information early in development. This is true despite the fact that both modalities are important to speech comprehension even at early stages of language acquisition. How then do listeners learn how to process auditory and visual information as part of a unified signal? In the auditory domain, statistical learning processes provide an excellent mechanism for acquiring phonological categories. Is this also true for the more complex problem of acquiring audiovisual correspondences, which require the learner to integrate information from multiple modalities? In this paper, we present simulations using Gaussian mixture models (GMMs) that learn cue weights and combine cues on the basis of their distributional statistics. First, we simulate the developmental process of acquiring phonological categories from auditory and visual cues, asking whether simple statistical learning approaches are sufficient for learning multi-modal representations. Second, we use this time course information to explain audiovisual speech perception in adult perceivers, including cases where auditory and visual input are mismatched. Overall, we find that domain-general statistical learning techniques allow us to model the developmental trajectory of audiovisual cue integration in speech, and in turn, allow us to better understand the mechanisms that give rise to unified percepts based on multiple cues.},
author = {Getz, Laura M. and Nordeen, Elke R. and Vrabic, Sarah C. and Toscano, Joseph C.},
doi = {10.3390/brainsci7030032},
file = {:Users/darrylhannan/Library/Application Support/Mendeley Desktop/Downloaded/Getz et al. - 2017 - Modeling the development of audiovisual cue integration in speech perception.pdf:pdf},
issn = {20763425},
journal = {Brain Sciences},
keywords = {Audiovisual cues,Cue weighting,Mixture of gaussians,Multimodal representations,Speech development,Speech perception,Statistical learning},
number = {3},
pmid = {28335558},
title = {{Modeling the development of audiovisual cue integration in speech perception}},
volume = {7},
year = {2017}
}
@article{Holt2010,
abstract = {Speech perception (SP) most commonly refers to the perceptual mapping from the highly variable acoustic speech signal to a linguistic representation, whether it be phonemes, diphones, syllables, or words. This is an example of categorization, in that potentially discriminable speech sounds are assigned to functionally equivalent classes. In this tutorial, we present some of the main challenges to our understanding of the categorization of speech sounds and the conceptualization of SP that has resulted from these challenges. We focus here on issues and experiments that define open research questions relevant to phoneme categorization, arguing that SP is best understood as perceptual categorization, a position that places SP in direct contact with research from other areas of perception and cognition. Spoken syllables may persist in the world for mere tenths of a second. Yet, as adult listeners, we are able to gather a great deal of information from these fleeting acoustic signals. We may apprehend the physical location of the speaker, the speaker's gender, regional dialect, age, emotional state, or identity. These spatial and indexical factors are conveyed by the acoustic speech signal in parallel with the linguistic message of the speaker (Abercrombie, 1967). Although these factors are of much interest in their own right, speech perception (SP) most commonly refers to the perceptual mapping from acoustic signal to some linguistic representation, such as phonemes, diphones, syllables, words, and so forth. 1},
author = {Holt, Lori L and Lotto, Andrew J},
doi = {10.3758/APP.72.5.1218},
file = {:Users/darrylhannan/Library/Application Support/Mendeley Desktop/Downloaded/Holt, Lotto - Unknown - Speech perception as categorization.pdf:pdf},
journal = {Atten Percept Psychophys},
title = {{Speech perception as categorization}},
url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2921848/pdf/nihms-227031.pdf},
year = {2010}
}
@article{Cooke2006,
author = {Cooke, Martin and Barker, Jon and Cunningham, Stuart and Shao, Xu},
doi = {10.1121/1.2229005͔},
file = {:Users/darrylhannan/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - Unknown - LETTERS TO THE EDITOR.pdf:pdf},
journal = {The Journal of the Acoustical Society of America},
number = {5},
title = {{An audio-visual corpus for speech perception and automatic speech recognition}},
url = {http://laslab.org/upload/an{\_}audio-visual{\_}corpus{\_}for{\_}speech{\_}perception{\_}and{\_}automatic{\_}speech{\_}recognition.pdf},
volume = {20},
year = {2006}
}
@article{McGurk1976,
author = {McGurk, Harry and MacDonald, John},
file = {:Users/darrylhannan/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - Unknown - Hearing Lips and Seeing Voices.pdf:pdf},
journal = {Nature Publishing Group},
pages = {746--748},
title = {{Hearing Lips and Seeing Voices}},
url = {http://usd-apps.usd.edu/coglab/schieber/psyc707/pdf/McGurk1976.pdf},
volume = {264},
year = {1976}
}
